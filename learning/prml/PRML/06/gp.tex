\documentclass[a4paper]{article}
\usepackage{xeCJK}
\setCJKmainfont{STKaiti}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Gaussian Process classification(GPC)}
\author{}
\maketitle

Supervised learning can be divided into regression and classification problems. outputs for classification are \emph{discrete class labels}, regression is concerned with the \emph{prediction of continuous quantities}.

\section{Regression}
One can think of a Gaussian process as defining a distribution over functions, and inference taking place directly in the space of functions, the \emph{function-space view}

\subsection{Weight-space View}
Bayesian treatment of the linear model. Enhancement to this class of models by projecting the inputs into a high-dimensional \emph{feature space} and applying the linear model.

\textbf{Training set} $\mathcal{D}$ of $n$ observations, $\mathcal{D} = \{(\mathbf{x}_i, y_i)|i = 1,...,n\}$, $\mathbf{x}$ denotes an input vector (covariates) of dimension $D$ and $y$ denotes a scalar output or target (dependent variable). the column vector inputs for all $n$ cases are aggregated in the $D \times n$ \emph{design matrix} $X$, $\mathcal{D} = \{X,\mathbf{y}\}$

\subsubsection{The Standard Linear Model}

\begin{align}
  f(\mathbf{x}) = \mathbf{x}^T\mathbf{w},\;y = f(\mathbf{x}) + \epsilon
\end{align}

\begin{align}
  \epsilon \sim \mathcal{N}(0,\sigma_n^2)
\end{align}

\emph{likelihood}, the probability density of the observations given the parameters, which is factored over cases in the training set
\begin{align}
  p(\mathbf{y}|X,\mathbf{w}) = \prod p(y_i|\mathbf{x}_i,\mathbf{w}) = \mathcal{N}(X^T\mathbf{w},\sigma_n^2 I)
\end{align}

\subsubsection{Projections of Inputs into Feature Space}

first project the inputs into some high dimensional space using a set of \emph{basis functions} and then apply the linear model in this space instead of directly on the inputs themselves. e.g. $\phi(x) = (1, x, x^2, x^3,...)^T$, \emph{the model is still linear in the parameters}. Now the model is
\begin{align}
  f(x) = \phi(x)^T \mathbf{w}
\end{align}
\emph{disign matrix} $\Phi(X)$ is substituted for $X$.

predictive distribution


\subsection{Function-space View}

\textbf{Definition} A Gaussian process is \emph{a collection of random variables, any finite number of which have a joint Gaussian distribution}.
A Gaussian process $f(\mathbf{x})$ is completely specified by its mean function $m(\mathbf{x})$ and covariance function $k(\mathbf{x},\mathbf{x}')$.
\begin{align*}
  m(\mathbf{x}) &= \mathbb{E}[f(\mathbf{x})]\\
  k(\mathbf{x},\mathbf{x}') &= \mathbb{E}[f(\mathbf{x}-m(\mathbf{x}))\: f(\mathbf{x}'-m(\mathbf{x}'))]
\end{align*}
Gaussian process as
\begin{align}
  f(\mathbf{x}) \sim \mathscr{G} \mathcal{P}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
\end{align}

A simple example of a Gaussian process can be obtained from our Bayesian linear regression model$f(x) = \phi(x)^T\mathbf{w}$ with prior $\mathbf{w} \sim \mathcal{N}(0,\Sigma_p)$

\begin{align*}
  \mathbb{E}[f(x)] &= \phi(\mathbf{x})^T \mathbb{E}[w] = 0\\
  \mathbb{E}[f(x)f'(x)] &= \phi(x)^T \Sigma_p \phi(x')
\end{align*}

\emph{squared exponential}
\begin{align*}
  cov[f(x_p), f(x_q)] = k(x_p,x_q) = exp(-\frac{1}{2} |x_p - x_q|^2)
\end{align*}

\begin{align}
  f_* \sim \mathcal{N} (0, K(X_*, X_*))
\end{align}

\paragraph{Prediction with Noise-free Observations}
Consider the simple special case where the observations are noise free $\{(\mathbf{x_i},f_i)| i=1,...,n\}$
The joint distribution of the training outputs $f$ and the test outputs $f_*$ is
\begin{align}
  \begin{bmatrix}
    f\\
    f_*
  \end{bmatrix}
  \sim \mathcal{N}(0,
  \begin{bmatrix}
    K(X,X) & K(X,X_*)\\
    K(X_*,X) & K(X_*,X_*)\\
  \end{bmatrix})
\end{align}
$K(X, X_*)$ denotes the $n \times n_*$ matrix of the covariances evaluated at all pairs of training and test points.

\begin{align}
\label{eq:joint_posterior}
  f_*|X_*,X,f \sim \mathcal{N} (K(X_*,X) K(X_,X)^{-1} \mathbf{f},\: K(X_*,X_*) - K(X_*,X) K(X,X)^{-1} K(X,X_*) )
\end{align}

Function values $f_*$ can be sampled from the joint posterior distribution by evaluating the mean and covariance matrix from \cite{eq:joint_posterior} \refeq{eq:joint_posterior}

\paragraph{Prediction using Noisy Observations}

\begin{align}
  \begin{bmatrix}
    f\\
    f_*
  \end{bmatrix}
  \sim \mathcal{N}(0,
  \begin{bmatrix}
    K(X,X) + \sigma_n^2 I & K(X,X_*)\\
    K(X_*,X) & K(X_*,X_*)\\
  \end{bmatrix})
\end{align}
predictive
\begin{align*}
  \mathbf{f}_*|X,y,X_* &\sim \mathcal{N}(\hat{f}_*,\hat{cov{f_*}})\\
  \hat{\mathbf{f}}_* &= K(X_*,X)[K(X,X)+\sigma_n^2I]^{-1} \mathbf{y}\\
  cov(\mathbf{f}_*) &= K(X_*,X_*) - K(X_*,X)[K(X_*,X)+\sigma_n^2I]^{-1} K(X,X_*)
\end{align*}

\section{Classification}

For classification models, where the targets are discrete class labels, the Gaussian likelihood is inappropriate.
In the classification case the likelihood is non-Gaussian but the posterior process can be \emph{approximated} by a GP.

\subsection{Classification Problems}
natural starting point for discussing joint probability $p(y, x)$, where $y$ denotes the class label. Using Bayes’ theorem this joint probability can be decomposed either as $p(y)p(x|y)$ or as $p(x)p(y|x)$. This gives rise to two different approaches:
\emph{generative}: model the class-conditional probability $p(x|y)$ and poior $p(y)$ for each class, and then compute the posterior
\begin{align}
  p(y|\mathbf{x}) = { p(y)p(\mathbf{x}|y) \over p(\mathbf{x})} = {p(y)p(\mathbf{x}|y) \over \sum_C p(C_k)p(\mathbf{x}|C_k) }
\end{align}
\emph{discriminative}: model $p(y|x)$ directly.
For \textbf{generative} case, model the class-conditional densities with Gaussians: $p(x|C_k) = N(\mu_k, \Sigma_k)$
For \textbf{discriminative} case, model the class-conditional densities with \emph{linear logistic regression}: $p(x|C_k) = \lambda(\mathbf{x}^T\mathbf{w})$, $\lambda(x) = {1 \over 1+exp(-x)} $

\subsubsection{Decision Theory for Classification}
Predictive probabilities $p(y_* |x_*)$ for a test input $x_*$,

\subsection{Gaussian Process Classification}
For binary classification the basic idea behind Gaussian process prediction:
place a GP prior over the \emph{latent function} $f(x)$ and then squash this through the logistic function to obtain a prior on $\pi(x) \gets  p(y=+1|x) = \sigma(f(x))$.

Inference is naturally divided into two steps:
1. computing the distribution of the latent variable corresponding to a test case
\begin{align}
\label{eq:latent_dist}
  p(f_*|X,y,x_*) = \int p(f_*|X,x_*,f)p(f|X,y) df
\end{align}
$p(f|X,y) = p(y|f)p(f|X)/p(y|X)$ is the posterior over the latent variables.

2. using this distribution over the latent $f_*$ to produce a probabilistic prediction
\begin{align}
  \hat{\pi}_* \gets p(y_*=+1|X,y,x_*) = \int \sigma(f_*)p(f_*|X,y,x_*) df_*
\end{align}


\subsection{The Laplace Approximation for the Binary GP Classifier}
Laplace’s method utilizes a Gaussian approximation $q(f|X, y)$ to the posterior $p(f|X, y)$ in the integral \ref{eq:latent_dist}
\begin{align}
\label{eq:gaussian_appro}
  q(\mathbf{f}|X,\mathbf{y}) = \mathcal{N}(\mathbf{f}|\mathbf{\hat{f}}, A^{-1}) \propto exp(-\frac{1}{2} (\mathbf{f} - \mathbf{\hat{f}})^T A (\mathbf{f} - \mathbf{\hat{f}})
\end{align}
where $\hat{f} = argmax_f p(f|X, y)$, and $A = -\nabla\nabla log p(f|X, y)|_{f=\hat{f}}$ is the Hessian of the negative log posterior at that point.

\subsubsection{Posterior}
\label{sec:3.4.1}

By Bayes's rule the posterior over the latent variables is given by $p(\mathbf{f}|X,\mathbf{y}) = p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|X)/p(\mathbf{y}/X)$, $p(\mathbf{y}|X)$ is independent of $\mathbf{f}$, $\log(\mathbf{y}|X) = -\frac{1}{2} \mathbf{f}^T K^{-1} \mathbf{f} - \frac{1}{2} \log |K| - \frac{n}{2} \log 2\pi$
\begin{align*}
\label{eq:logposterior}
  \Psi(\mathbf{f}) &\gets \log p(\mathbf{y}|\mathbf{f}) + \log p(\mathbf{f}|X) \\
  &= \log p(\mathbf{y}|\mathbf{f}) -\frac{1}{2} \mathbf{f}^T K^{-1} \mathbf{f} - \frac{1}{2} \log |K| - \frac{n}{2} \log 2\pi
\end{align*}
Differentiating \ref{eq:logposterior},
\begin{align*}
  \nabla \Psi(\mathbf{f}) &= \nabla \log p(\mathbf{y}|\mathbf{f}) - K^{-1}\mathbf{f}\\
  \nabla\nabla \Psi(\mathbf{f}) &= \nabla\nabla \log p(\mathbf{y}|\mathbf{f}) - K^{-1} = -W - K^{-1}
\end{align*}
where $W = \nabla\nabla \log p(\mathbf{y}|\mathbf{f})$ is diagonal

\begin{align}
  \nabla \Psi(\mathbf{f}) = 0 \implies \hat{\mathbf{f}} = K(\nabla p(\mathbf{y}|\hat{\mathbf{f}}))
\end{align}
Using Newton's method
\begin{align*}
  \mathbf{f}^{new} &= \mathbf{f} -   \nabla\nabla\Psi^{-1} \nabla\Psi\\
  &= (K^{-1}+W)^{-1} (W \mathbf{f} + \nabla \log p(\mathbf{y}|\mathbf{f}))
\end{align*}
break $\mathbf{f}$ into two subvectors $f_1$ not well-explained and $f_2$ well-explained.
\begin{align*}
  f_1^{new} &= K_{11}(I_{11}+W_{11}K_{11})^{-1} (W_{11}f_1 + \nabla \log p(y_1|f_1))\\
  f_2^{new} &= K_{21}K_{11}^{-1} f_1^{new}
\end{align*}
where $K_{21}$ denotes the $n_2 \times n_1$ block of $K$ containing
$f_1^{new}$ is computed by ignoring entirely the well-explained points($\nabla \log p(y|f) \simeq 0$), and $f_2^{new}$ is predicted from $f_1^{new}$ using the usual GP prediction methods

Having found the maximum posterior $\mathbf{\hat{f}}$
\begin{align*}
  q(\mathbf{f}|X,\mathbf{y}) = \mathcal{N}(\mathbf{\hat{f}}, (K^{-1}+W)^{-1})
\end{align*}

\subsubsection{Predictions}
The posterior mean for $f_*$
\begin{align*}
  \mathbb{E}_q[f_*|X,y,x_*] &= k(x_*)^TK^{-1}\hat{\mathbf{f}} = k(x_*)^T \nabla \log p(\mathbf{y}|\mathbf{\hat{f}}\\
  \mathbb{E}_q[f_*|X,y,x_*] &= k(x_*)^TK^{-1} \mathbb{E}[\mathbf{f}|X,\mathbf{y}]
\end{align*}
The variance of $f_*|W,y$
\begin{align*}
  \mathbb{V}_q[f_*|X,y,x_*] &= \mathbb{E}_{p(f_*)} [(f_* - \mathbb{E}[f_*|X,x_*,f])]\\
  &+ \mathbb{E}_q [(\mathbb{E}[f_*|X,x_*,f]-\mathbb{E}[f_*|X,y,x_*])^2]\\
  \mathbb{V}_q[f_*|X,y,x_*] &= k(x_*,x_*)-k_*^T(K-W^{-1})^{-1}k_*
\end{align*}

\begin{align*}
  \bar{\pi}_* = \mathbb{E}_q[\pi_*|X,y,x_*] = \int \sigma(f_*)q(f_*|X,y,x_*) df_*
\end{align*}
the sigmoid of the expectation of $\mathbf{f}: \hat{\pi}_* = \sigma(\mathbb{E}_q[f_*|\mathbf{y}])$, \textit{MAP prediction}
note that the decision boundary using the MAP value $E_q[f_*|X,y,x_*]$ corresponds to $\sigma(E_q) = 1/2$ or $\sigma(E_q) = 0$.

\end{document}
