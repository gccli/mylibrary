\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Kernel Method}
\author{}
\maketitle

Many linear parametric models can be re-cast into an equivalent
 'dual representation' in which the predictions are also based on linear
 combinations of a kernel function evaluated at the training data points.
As we shall see, for models which are based on a fixed nonlinear feature
 space mapping $\phi(x)$, the kernel function is given
\begin{align}
\mathbf{k}(\mathbf{x}, \mathbf{x'}) = \phi(x)^T\phi(x')
\end{align}

\section{Dual Representations}
Many linear models for regression and classification can be reformulated in terms of
 a dual representation in which the kernel function arises naturally.
Here we consider a linear regression model whose parameters are determined by
minimizing a regularized sum-of-squares error function given by
\begin{align}
J(w) = \frac{1}{2} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}^2 + \frac{\lambda}{2}w^tw
\end{align}
If we set the gradient of $J(w)$ with respect to $\mathbf{w}$ equal to zero,
 we see that the solution for $\mathbf{w}$ takes the form of a linear
 combination of the vectors $\phi(x_n)$
\begin{align}
w = -\frac{1}{\lambda} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}\phi(x_n)
  = \sum_{n=1}^Na_n\phi(x_n) = \mathbf{\Phi}(x_n)\mathbf{a_n}
\end{align}
The nth row of $\mathbf{\Phi}$ is given by $\phi(x_n)^T$, $\mathbf{a}
= (a_1,...,a_n)$
\begin{align}
a_n = -\frac{1}{\lambda} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}
\end{align}
Instead of working with the parameter vector $\mathbf{w}$, we can now
 reformulate the leastsquares algorithm in terms of the parameter vector
 $\mathbf{a}$, giving rise to a \textit{dual representation}.

\begin{align}
J(\mathbf{a}) = \frac{1}{2} \mathbf{a}^T \mathbf{\Phi} \mathbf{\Phi}^T
  \mathbf{\Phi} \mathbf{\Phi}^T \mathbf{a} - \mathbf{a}^T
  \mathbf{\Phi} \mathbf{\Phi}^T \mathbf{t} + \frac{1}{2} \mathbf{t}^T
  \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T \mathbf{\Phi}
  \mathbf{\Phi}^T \mathbf{a}
\end{align}
where $t = (t_1,...,t_N)^T$, We now define the \textit{Gram matrix}
$\mathbf{K} = \mathbf{\Phi} \mathbf{\Phi}^T$, which is an N × N symmetric matrix with elements
\begin{align}
\mathit{K}_{nm} = \phi(x_n)^T\phi(x_m) = k(x_n, x_m)
\end{align}

\begin{align}
J(\mathbf{a}) = \frac{1}{2} \mathbf{a}^T \mathbf{K} \mathbf{K} \mathbf{a} - \mathbf{a}^T
  \mathbf{K} \mathbf{t} + \frac{1}{2} \mathbf{t}^T
  \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T \mathbf{K} \mathbf{a}
\\
\mathbf{a} = (\mathbf{K} + \lambda \mathit{\mathbf{I}_N})^{-1} \mathbf{t}
\\
y(x) = w^T\phi(x) = a^T\Phi\phi(x) = \mathbf{k}(x)^T (\mathbf{K} +
  \lambda \mathit{\mathbf{I}_N})^{-1} \mathbf{t}
\end{align}
where we have defined the vector $\mathbf{k}(x)$ with elements $k_n(x) = k(x_n, x)$.

\section{Constructing Kernels}
Another commonly used kernel takes the form
\begin{align}
k(x, x') = exp(-\frac{\|x-x'\|^2}{2\sigma^2})\\
\|x-x'\|^2 = x^Tx+(x')^Tx' -2x^Tx'
\end{align}

One way to combine them is to use a generative model to define a kernel, and then use this kernel in a discriminative approach.
Given a generative model $p(x)$ we can define a kernel by
\begin{align}
k(x, x') = p(x)p(x')\\
k(x, x') = \sum_{i}p(x|i)p(x'|i)p(i)\\
k(x, x') = \int p(x|z)p(x'|z)p(z)dz
\end{align}
Now suppose that our data consists of ordered sequences of length $L$
so that an observation is given by $\mathbf{X} = {x_1,...,x_L}$. A
popular generative model for sequences is the hidden Markov
model. which expresses the distribution $p(\mathbf{X})$ as a
marginalization over a corresponding sequence of hidden states
$\mathbf{Z} = {z_1,...,z_L}$. We can use this approach to define a
kernel function measuring the similarity of two sequences $X$ and $X'$
by extending the mixture representation
\begin{align}
k(X, X') = \sum_{Z}p(X|Z)p(X'|Z)p(Z)
\end{align}

An alternative technique for using generative models to define kernel
functions is known as the \textit{Fisher kernel}. Consider a
parametric generative model $p(x|\theta)$ where $\theta$ denotes the
vector of parameters. The goal is to find a kernel that measures the
similarity of two input vectors $x$ and $x'$ induced by the generative model.
\textit{Fisher score}
\begin{align}
g(\theta, \mathbf{x}) = \nabla_\theta ln p(\mathbf{x}|\theta)\\
k(x, x') = g(\theta, \mathbf{x})^T F^{-1} g(\theta, \mathbf{x'})\\
F = \mathbb{E}_x [g(\theta, \mathbf{x}) g(\theta, \mathbf{x})^T]
\textit{ (Fisher information matrix)}
\end{align}
the expectation is with respect to $x$ under the distribution $p(x|\theta)$.
In practice, it is often infeasible to evaluate the Fisher information
matrix. One approach is simply to replace the expectation in the
definition of the Fisher information with the sample average

A final example of a kernel function is the sigmoidal kernel given by
\begin{align}
k(x,x') = tanh(a \mathbf{x}^T \mathbf{x}' + b)
\end{align}

\section{Radial Basis Function Networks}
One choice that has been widely used is that of radial basis
functions, which have the property that each basis function depends
only on the radial distance (typically Euclidean) from a centre
$\mu_j$ , so that $\phi_j(x) = h(\| − \mu_j\|) $.

\subsection{Nadaraya-Watson model}

\section{Gaussian Processes}

\subsection{Linear regression revisited}
Consider a model deﬁned in terms of a linear combination of $M$ ﬁxed
basis functions given by the elements of the vector $\phi(x)$ so that
\begin{align}
y(x) = \mathbf{w}^T\phi(\mathbf{x})\\
p(\mathbf{w}) = \mathcal{N} (\mathbf{w}|0, \alpha^{-1}\mathit{I})
\end{align}
we denote by the vector $\mathbf{y}$ with elements $y_n = y(x_n)$ for $n = 1,...,N$
\begin{align}
\mathbf{y} = \Phi \mathbf{w} \text{ $(\Phi_{nk} = \phi_k(x_n)) $ }\\
\mathbb{E}[y] = 0\\
cov[y] = \mathbb{E}[yy^T] = \Phi \mathbb{E}[ww^T] \Phi^T=
  \frac{1}{\alpha}\Phi\Phi^T = \mathbf{K}
\end{align}
where $K$ is the Gram matrix with elements
\begin{align}
\mathit{K}_{nm} = k(x_n, x_m) = \frac{1}{\alpha} \phi(x_n)^T\phi(x_m)
\end{align}

We can also deﬁne the kernel function directly, rather than indirectly
through a choice of basis function. Figure 6.4 shows samples of functions drawn from Gaussian processes for two different choices of kernel function.

\subsection{Gaussian processes for regression}
Apply Gaussian process models to regression, we need to take account of the noise on the observed target values, which are given by
\begin{align}
t_n = y_n + \epsilon_n
\end{align}
where $yn = y(x_n)$, and $\epsilon_n$ is a random noise variable whose
value is chosen independently for each observation $n$. Here we shall
consider noise processes that have a Gaussian distribution, so that
\begin{align}
p(t_n|y_n) =  \mathcal{N}(t_n|y_n, \beta^{-1})\\
p(\mathbf{t}|\mathbf{y}) =  \mathcal{N}(\mathbf{t}|\mathbf{y}, \beta^{-1}\mathit{I}_N)
\end{align}
From the deﬁnition of a Gaussian process, the marginal distribution
$p(\mathbf{y})$ is given by a Gaussian whose mean is zero and whose covariance is deﬁned by a Gram matrix $\mathbf{K}$ so that
\begin{align}
p(\mathbf{y}) = \mathcal{N}(\mathbf{y}|0, \mathbf{K})
\end{align}
The kernel function that determines $K$ is typically chosen to express the property
that, for points $x_n$ and $x_m$ that are similar, the corresponding
values $y(x_n)$ and $y(x_m)$ will be more strongly correlated than for
dissimilar points. Here the notion of similarity will depend on the application.
the marginal distribution of $\mathbf{t}$ is given by
\begin{align}
p(\mathbf{t}) = \int p(t|y)p(y)dy = \mathcal{N}(\mathbf{t}|0, \mathbf{C})\\
C(x_n, x_m) = k(x_n, x_m) + \beta^{-1} \delta_{nm}
\end{align}
One widely used kernel function for Gaussian process regression is
given by the exponential of a quadratic form, with the addition of
constant and linear terms to give
\begin{align}
k(x_n, x_m) = \theta_0 exp\{ -\frac{\theta_1}{2}\|x_n-x_m\|^2\} +
  \theta_2 + \theta_3x_n^Tx_m
\end{align}

\begin{align}
p(\mathbf{t}_{N+1}) = N(t_{N=1}|0, C_{N+1})\\
C_{N+1} =
 \begin{pmatrix}
  C_{N} & \mathbf{k}\\
  \mathbf{k}^T & c
 \end{pmatrix}
\end{align}


\begin{align}
m(x_{N+1}) = \mathbf{k}^TC_N^{-1}\mathbf{t}\\
\sigma^2(x_{N+1}) = c-\mathbf{k}^TC_N^{-1}\mathbf{k}
\end{align}
The central computational operation in using Gaussian processes will
involve the inversion of a matrix of size $N \times N$ , for which
standard methods require $O(N^3)$ computations.

\subsection{Learning the hyperparameters}
The predictions of a Gaussian process model will depend, in part, on
the choice of covariance function. In practice, rather than ﬁxing the covariance function, we
may prefer to use a parametric family of functions and then infer the
parameter values from the data. These parameters govern such things as
the length scale of the correlations and the precision of the noise
and correspond to the hyperparameters in a standard parametric model.
Techniques for learning the hyperparameters are based on the
evaluation of the likelihood function $p(t|\theta)$ where $\theta$
denotes the hyperparameters of the Gaussian process model.
The log likelihood function for a Gaussian process regression model

\begin{align}
ln p(t|\theta) = - \frac{1}{2} ln |C_N| - \frac{1}{2} t^TC_N^{-1}t - \frac{N}{2} ln(2\pi)\\
\frac{\partial}{\partial \theta_i} ln p(t|\theta) = - \frac{1}{2}
  Tr(C_N^{-1}\frac{\partial C_N}{\partial \theta_i}) + \frac{1}{2}
  t^TC_N^{-1} \frac{\partial C_N}{\partial \theta_i} C_N^{-1}t
\end{align}

\subsection{Automatic relevance determination}
Consider a Gaussian process with a two-dimensional input space $x =
(x_1, x_2)$, having a kernel function of the form
\begin{align}
k(\mathbf{x}, \mathbf{x}') = \theta_0 exp \{-\frac{1}{2}
  \sum_{i=1}^{2} \mu^i(x_i - x_i')^2 \}
\end{align}

\subsection{Gaussian processes for classiﬁcation}
Consider ﬁrst the two-class problem with a target variable $t \in {0,
  1}$. If we deﬁne a Gaussian process over a function $a(x)$ and then
transform the function using a logistic sigmoid $y = \sigma(a)$, then
we will obtain a non-Gaussian stochastic process over functions $y(x)$ where $y \in (0, 1)$.
target variable $t$ is then given by the Bernoulli distribution
\begin{align}
p(t|a) = \sigma(a)^t(1-\sigma(a)^{1-t}\\
p(a_{N+1} = \mathcal{N}(a_{N+1}|0, C_{N+1})\\
C(x_n, x_m) = k(x_n, x_m) + \nu \delta_{nm}
\end{align}
predictive distribution is given by
\begin{align}
p(t_{N+1} = 1|t_N) = \int (t_N+1 = 1|a_{N+1})p(a_{N+1}|t_N )da_{N +1}
\end{align}
where $p(t_{N+1} = 1|a_{N +1}) = \sigma(a_{N +1})$.
This integral is analytically intractable, and so may be approximated
using sampling methods. Alternatively, we can consider techniques
based on an analytical approximation.

\subsection{Laplace approximation}
In order to evaluate the predictive distribution, we seek a Gaussian
approximation to the posterior distribution over $a_{N+1}$ , which using Bayes' theorem, is given by
\begin{align*}
p(a_{N+1}|\mathbf{t}_N) &= \int p(a_{N+1},\mathbf{a}_N|\mathbf{t}_N)d\mathbf{a}_N\\
&= \frac{1}{p(\mathbf{t}_N)} \int
  p(a_{N+1}|\mathbf{a}) p(\mathbf{t}_N|a_{N+1},\mathbf{a}_N) d \mathbf{a}_N\\
&= \frac{1}{p(\mathbf{t}_N)} \int
  p(a_{N+1}|\mathbf{a})p(\mathbf{a}_N)  p(\mathbf{t}_N|\mathbf{a}_N)d\mathbf{a}_N\\
&= \int p(a_{N+1}|\mathbf{a})  p(\mathbf{a}_N|\mathbf{t}_N)d\mathbf{a}_N
\end{align*}
where $p(\mathbf{t}_N|a_{N+1},\mathbf{a}_N) = p(\mathbf{t}_N|\mathbf{a}_N) $
\begin{align}
p(a_{N+1}|\mathbf{a}_N) = \mathcal{N} (a_{N+1}|\mathbf{k}^T
  C_{N}^{-1}\mathbf{a}_N, c-\mathbf{k}^T C_{N}^{-1}\mathbf{k})
\end{align}

\begin{align*}
p(\mathbf{t}_{N}|\mathbf{a}_N) &= \prod_{n=1}^N \sigma(a_n)^{t_n}(1-\sigma(a_n))^{1-t_n}\\
&= \prod_{n=1}^N e^{a_nt_n} \sigma(-a_n)
\end{align*}
We then obtain the Laplace approximation by Taylor expanding the
logarithm of $p(a_N|t_N)$

\begin{align*}
\Psi(\mathbf{a}_N) &= \mathsf{ln} p(\mathbf{a}_N) + \mathsf{ln} p(\mathbf{t}_{N}|\mathbf{a}_N)\\
&=-\frac{1}{2}\mathbf{a}_N^TC_N^{-1}\mathbf{a}_N
  -\frac{N}{2}\mathsf{ln}(2\pi) -\frac{1}{2}\mathsf{ln}|C_N| +
  \mathbf{t}_N^T\mathbf{a}_N - \sum_{n=1}^N \mathsf{ln} (1+e^{a_n}) + const
\end{align*}
find the mode of the posterior distribution, and this requires that we
evaluate the gradient of $\Psi(a_N)$:
\begin{align}
\nabla \Psi(\mathbf{a}_N) = \mathbf{t}_N - \sigma_N - C_N^{-1}\mathbf{a}_N
\end{align}
$\sigma_N$ is a vector with elements $\sigma(a_n)$, we cannot simply find the mode by
setting this gradient to zero, because $\sigma_N$ depends nonlinearly on $a_N$, and so we
resort to an iterative scheme based on the Newton-Raphson method,
iterative reweighted least squares (IRLS) algorithm, This requires the second
derivatives of $\Psi(a_N)$
\begin{align}
  \nabla\nabla \Psi(a_N) = -\mathbf{W}_N - \mathbf{C}_N^{-1}
\end{align}
where$W_N$ is a diagonal matrix with elements $\sigma(a_n)(1−\sigma(a_n))$.
Using the Newton-Raphson formula, the iterative update equation for $a_N$
\begin{align}
a_N^{new} = C_N(I+W_NC_N)^{-1} {T_N -\sigma_N + W_Na_N}
\end{align}
These equations are iterated until they converge to the mode which we
denote by $a_N^*$. At the mode, the gradient $\nabla \Psi(a_N)$ will
vanish, and hence $a_N^*$ will satisfy
\begin{align}
a_N^* = C_N(t_N - \sigma_N)
\end{align}

\begin{align}
\mathbf{H} = - \nabla\nabla\Psi(a_N) = \mathbf{W_N + C_N^{-1}}
\end{align}

\begin{align}
q(a_n) = \mathcal{N}(a_n|a_N^*, H^{-1})\\
\mathbb{E}[a_{N+1}|\mathbf{t}_N] = k^T(t_N - \sigma_N)\\
var[a_{N+1}|\mathbf{t}_N] = c - k^T(W_N^{-1} + C_N)^{-1} \mathbf{k}
\end{align}
We also need to determine the parameters $\theta$ of the covariance function.

\begin{align}
p(\mathbf{t}_N |\theta) = \int p(\mathbf{t}_N|\mathbf{a}_N) p(\mathbf{a}_N |\theta)  d\mathbf{a}_N
\end{align}
\begin{align}
\mathsf{ln}p(\mathbf{t}_N |\theta) = \Psi(\mathbf{a}_N^*) -
  \frac{1}{2}\mathsf{ln}|\mathbf{W}_N+C_N^{-1}| + \frac{N}{2} \mathsf{ln}(2\pi)
\end{align}
where $\Psi(\mathbf{a}_N^*) = \mathsf{ln} p(\mathbf{a}_N^* |\theta) + p(\mathbf{t}_N |\mathbf{a}_N^*)$




\end{document}
