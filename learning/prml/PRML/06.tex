\documentclass[a4paper]{article}
\usepackage{xeCJK}
\setCJKmainfont{STKaiti}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Kernel Method}
\author{}
\maketitle

Many linear parametric models can be re-cast into an equivalent \emph{dual representation} in which the predictions are also based on linear combinations of a \emph{kernel} function evaluated at the training data points. As we shall see, for models which are based on a fixed nonlinear feature space mapping $\phi(x)$, the kernel function is given
\begin{align}
\mathbf{k}(\mathbf{x}, \mathbf{x'}) = \phi(x)^T\phi(x')
\end{align}

用特征空间的内积的方式表示核的概念使得我们能够对许多著名的算法进行有趣的扩展。扩展的方法是使用\emph{核技巧}(kernel trick),也被称为\emph{核替换}(kernel substitution)

\section{Dual Representations}
正则化平方误差函数
\begin{align}
J(w) = \frac{1}{2} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}^2 + \frac{\lambda}{2}w^tw
\end{align}
Set the gradient of $J(w)$ with respect to $\mathbf{w}$ equal to zero, we see that the solution for $\mathbf{w}$ takes the form of a linear combination of the vectors $\phi(x_n)$
\begin{align}
w = -\frac{1}{\lambda} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}\phi(x_n)
  = \sum_{n=1}^Na_n\phi(x_n) = \mathbf{\Phi}(x_n)\mathbf{a_n}
\end{align}
$\mathbf{a} = (a_1,...,a_n)^T$
\begin{align}
a_n = -\frac{1}{\lambda} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}
\end{align}
Instead of working with the parameter vector $\mathbf{w}$, we can now reformulate the leastsquares algorithm in terms of the parameter vector $\mathbf{a}$, giving rise to a \textit{dual representation}.

\begin{align}
J(\mathbf{a}) = \frac{1}{2} \mathbf{a}^T \mathbf{\Phi} \mathbf{\Phi}^T
  \mathbf{\Phi} \mathbf{\Phi}^T \mathbf{a} - \mathbf{a}^T
  \mathbf{\Phi} \mathbf{\Phi}^T \mathbf{t} + \frac{1}{2} \mathbf{t}^T
  \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T \mathbf{\Phi}
  \mathbf{\Phi}^T \mathbf{a}
\end{align}
where $t = (t_1,...,t_N)^T$, We now define the \textit{Gram matrix} $\mathbf{K} = \mathbf{\Phi} \mathbf{\Phi}^T$, which is an $N \times N$ symmetric matrix with elements
\begin{align}
\mathit{K}_{nm} = \phi(x_n)^T\phi(x_m) = k(x_n, x_m)
\end{align}

\begin{align}
J(\mathbf{a}) = \frac{1}{2} \mathbf{a}^T \mathbf{K} \mathbf{K} \mathbf{a} - \mathbf{a}^T
  \mathbf{K} \mathbf{t} + \frac{1}{2} \mathbf{t}^T
  \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T \mathbf{K} \mathbf{a}
\\
\mathbf{a} = (\mathbf{K} + \lambda \mathit{\mathbf{I}_N})^{-1} \mathbf{t}
\\
y(\mathbf{x}) = w^T\phi(\mathbf{x}) = a^T\Phi\phi(\mathbf{x}) = \mathbf{k}(\mathbf{x})^T (\mathbf{K} + \lambda \mathit{\mathbf{I}_N})^{-1} \mathbf{t}
\end{align}
where we have defined the vector $\mathbf{k}(x)$ with elements $k_n(x) = k(x_n, x)$.
\emph{注意,在$x$处的预测由训练集数据的目标值的线性组合给出}

work directly in terms of kernels and avoid the explicit introduction of the feature vector $\phi(x)$, which allows us implicitly to use feature spaces of high, even infinite, dimensionality.

\section{Constructing Kernels}
Another commonly used kernel takes the form
\begin{align}
k(x, x') = exp(-\frac{\|x-x'\|^2}{2\sigma^2})\\
\|x-x'\|^2 = x^Tx+(x')^Tx' -2x^Tx'
\end{align}

One way to combine them is to use a generative model to define a kernel, and then use this kernel in a discriminative approach.
Given a generative model $p(x)$ we can define a kernel by
\begin{align}
k(x, x') = p(x)p(x')\\
k(x, x') = \sum_{i}p(x|i)p(x'|i)p(i)\\
\end{align}

\subparagraph{概率生成式模型}
\begin{align*}
  k(x, x') = p(x)p(x')\\
  k(x, x') = \sum_i p(x|i)p(x'|i)p(i)\\
  k(x, x') = \int p(x|z)p(x'|z)p(z)dz\\
\end{align*}

\emph{Hidden Markov Model}
Observation is given by $\mathbf{X} = {x_1,...,x_L}$, which expresses the distribution $p(\mathbf{X})$ as a marginalization over a corresponding sequence of \emph{hidden states} $\mathbf{Z} = {z_1,...,z_L}$. We can use this approach to define a kernel function measuring the similarity of two sequences $X$ and $X'$ by extending the mixture representation
\begin{align}
k(X, X') = \sum_{Z}p(X|Z)p(X'|Z)p(Z)
\end{align}

{Fisher kernel}
Consider a parametric generative model $p(x|\theta)$ where $\theta$ denotes the vector of parameters. The goal is to find a kernel that measures the similarity of two input vectors $x$ and $x'$ induced by the generative model. consider the gradient with respect to $\theta$, which defines a vector in a feature space, particular, consider \textit{Fisher score}
\begin{align*}
  g(\theta, \mathbf{x}) &= \nabla_\theta ln p(\mathbf{x}|\theta) \\
  k(x, x') &= g(\theta, \mathbf{x})^T F^{-1} g(\theta, \mathbf{x'}) \textit{ (Fisher kernel)}\\
  F &= \mathbb{E}_x [g(\theta, \mathbf{x}) g(\theta, \mathbf{x})^T] \textit{ (Fisher information matrix)}
\end{align*}
the expectation is with respect to $x$ under the distribution $p(x|\theta)$. In practice, it is often infeasible to evaluate the Fisher information matrix. One approach is simply to replace the expectation in the definition of the Fisher information with the sample average

\emph{sigmoidal kernel}
\begin{align}
k(x,x') = tanh(a \mathbf{x}^T \mathbf{x}' + b)
\end{align}

\section{Radial Basis Function Networks}
Widely used basis function: \emph{radial basisfunctions}, which have the property that each basis function depends only on the radial distance (typically Euclidean) from a centre $\mu_j$ , so that $\phi_j(x) = h(\| − \mu_j\|) $.
输入变量$\mathbf{x}$上的噪声由一个服从分布$\nu(\xi)$的变量$\xi$描述，平方误差和（sum-of-squares error）函数：
\begin{align*}
  E &= {1 \over 2} \sum_{n=1}^N \int \{y(\mathbf{x}_n + \xi) - t_n\} \nu(\xi) \mathrm{d} \xi\\
  y(\mathbf{x}) &= \sum_{n=1}^N t_nh(\mathbf{x}-\mathbf{x}_n)\\
  h(\mathbf{x},\mathbf{x}_n) &= { \nu(\mathbf{x} - \mathbf{x}_n) \over sum_{n=1}^N \nu(\mathbf{x} - \mathbf{x}_n)} \textit{ (basic function)}
\end{align*}
basis function centred on every data point. This is known as the \emph{Nadaraya-Watson model}
\subsection{Nadaraya-Watson model}
use a Parzen density estimator to model the joint distribution $p(x, t)$
\begin{align}
  p(x, t) = {1 \over N} \sum_{n=1}^N f(\mathbf{x}-\mathbf{x}_n, t-t_n) \textit{ (component density function)}
\end{align}
 $f(x, t)$ centred on each data point.

\section{Gaussian Processes}
在高斯过程的观点中,我们抛弃参数模型,直接定义函数上的先验概率分布。

\subsection{Linear regression revisited}
Consider a model deﬁned in terms of a linear combination of $M$ ﬁxed basis functions given by the elements of the vector $\phi(x)$ so that
\begin{align}
y(x) = \mathbf{w}^T\phi(\mathbf{x})\\
p(\mathbf{w}) = \mathcal{N} (\mathbf{w}|0, \alpha^{-1}\mathit{I})
\end{align}
we denote by the vector $\mathbf{y}$ with elements $y_n = y(x_n)$ for $n = 1,...,N$
\begin{align}
\mathbf{y} = \Phi \mathbf{w} \text{ $(\Phi_{nk} = \phi_k(x_n)) $ }\\
\mathbb{E}[y] = 0\\
cov[y] = \mathbb{E}[yy^T] = \Phi \mathbb{E}[ww^T] \Phi^T=
  \frac{1}{\alpha}\Phi\Phi^T = \mathbf{K}
\end{align}
where $K$ is the Gram matrix with elements
\begin{align}
\mathit{K}_{nm} = k(x_n, x_m) = \frac{1}{\alpha} \phi(x_n)^T\phi(x_m)
\end{align}

通常来说,\textbf{高斯过程被定义为函数$y(x)$上的一个概率分布,使得在任意点集$x_1,...,x_N$处计算的$y(x)$的值的集合联合起来服从高斯分布}.
在输入向量$x$是二维的情况下,这也可以被称为高斯随机场(Gaussian random field).
可以用一种合理的方式为 $y(x_1),...,y(x_N)$赋予一个联合的概率分布,来确定一个随机过程(stochastic process) $y(x)$

\subsection{Gaussian processes for regression}
Apply Gaussian process models to regression, we need to take account of the noise on the observed target values, which are given by
\begin{align}
t_n = y_n + \epsilon_n
\end{align}
where $yn = y(x_n)$, and $\epsilon_n$ is a random noise variable whose
value is chosen independently for each observation $n$. Here we shall
consider noise processes that have a Gaussian distribution, so that
\begin{align}
p(t_n|y_n) =  \mathcal{N}(t_n|y_n, \beta^{-1})\\
p(\mathbf{t}|\mathbf{y}) =  \mathcal{N}(\mathbf{t}|\mathbf{y}, \beta^{-1}\mathit{I}_N)
\end{align}
From the deﬁnition of a Gaussian process, the marginal distribution
$p(\mathbf{y})$ is given by a Gaussian whose mean is zero and whose covariance is deﬁned by a Gram matrix $\mathbf{K}$ so that
\begin{align}
p(\mathbf{y}) = \mathcal{N}(\mathbf{y}|0, \mathbf{K})
\end{align}
The kernel function that determines $K$ is typically chosen to express the property that, for points $x_n$ and $x_m$ that are similar, the corresponding values $y(x_n)$ and $y(x_m)$ will be more strongly correlated than for dissimilar points. Here the notion of similarity will depend on the application. the marginal distribution of $\mathbf{t}$ is given by
\begin{align}
p(\mathbf{t}) = \int p(t|y)p(y)dy = \mathcal{N}(\mathbf{t}|0, \mathbf{C})\\
C(x_n, x_m) = k(x_n, x_m) + \beta^{-1} \delta_{nm}
\end{align}
One widely used kernel function for Gaussian process regression is given by the exponential of a quadratic form, with the addition of constant and linear terms to give
\begin{align}
k(x_n, x_m) = \theta_0 exp\{ -\frac{\theta_1}{2}\|x_n-x_m\|^2\} +
  \theta_2 + \theta_3x_n^Tx_m
\end{align}

\begin{align}
p(\mathbf{t}_{N+1}) = N(t_{N=1}|0, C_{N+1})\\
C_{N+1} =
 \begin{pmatrix}
  C_{N} & \mathbf{k}\\
  \mathbf{k}^T & c
 \end{pmatrix}
\end{align}


\begin{align}
m(x_{N+1}) = \mathbf{k}^TC_N^{-1}\mathbf{t}\\
\sigma^2(x_{N+1}) = c-\mathbf{k}^TC_N^{-1}\mathbf{k}
\end{align}
The central computational operation in using Gaussian processes will
involve the inversion of a matrix of size $N \times N$ , for which
standard methods require $O(N^3)$ computations.

\subsection{Learning the hyperparameters}
The predictions of a Gaussian process model will depend, in part, on
the choice of covariance function. In practice, rather than ﬁxing the covariance function, we
may prefer to use a parametric family of functions and then infer the
parameter values from the data. These parameters govern such things as
the length scale of the correlations and the precision of the noise
and correspond to the hyperparameters in a standard parametric model.
Techniques for learning the hyperparameters are based on the
evaluation of the likelihood function $p(t|\theta)$ where $\theta$
denotes the hyperparameters of the Gaussian process model.
The log likelihood function for a Gaussian process regression model

\begin{align}
ln p(t|\theta) = - \frac{1}{2} ln |C_N| - \frac{1}{2} t^TC_N^{-1}t - \frac{N}{2} ln(2\pi)\\
\frac{\partial}{\partial \theta_i} ln p(t|\theta) = - \frac{1}{2}
  Tr(C_N^{-1}\frac{\partial C_N}{\partial \theta_i}) + \frac{1}{2}
  t^TC_N^{-1} \frac{\partial C_N}{\partial \theta_i} C_N^{-1}t
\end{align}

\subsection{Automatic relevance determination}
Consider a Gaussian process with a two-dimensional input space $x =
(x_1, x_2)$, having a kernel function of the form
\begin{align}
k(\mathbf{x}, \mathbf{x}') = \theta_0 exp \{-\frac{1}{2}
  \sum_{i=1}^{2} \mu^i(x_i - x_i')^2 \}
\end{align}

\subsection{Gaussian processes for classiﬁcation}
Consider ﬁrst the two-class problem with a target variable $t \in {0,
  1}$. If we deﬁne a Gaussian process over a function $a(x)$ and then
transform the function using a logistic sigmoid $y = \sigma(a)$, then
we will obtain a non-Gaussian stochastic process over functions $y(x)$ where $y \in (0, 1)$.
target variable $t$ is then given by the Bernoulli distribution
\begin{align}
p(t|a) = \sigma(a)^t(1-\sigma(a)^{1-t}\\
p(a_{N+1} = \mathcal{N}(a_{N+1}|0, C_{N+1})\\
C(x_n, x_m) = k(x_n, x_m) + \nu \delta_{nm}
\end{align}
predictive distribution is given by
\begin{align}
p(t_{N+1} = 1|t_N) = \int (t_N+1 = 1|a_{N+1})p(a_{N+1}|t_N )da_{N +1}
\end{align}
where $p(t_{N+1} = 1|a_{N +1}) = \sigma(a_{N +1})$.
This integral is analytically intractable, and so may be approximated
using sampling methods. Alternatively, we can consider techniques
based on an analytical approximation.

\subsection{Laplace approximation}
In order to evaluate the predictive distribution, we seek a Gaussian
approximation to the posterior distribution over $a_{N+1}$ , which using Bayes' theorem, is given by
\begin{align*}
p(a_{N+1}|\mathbf{t}_N) &= \int p(a_{N+1},\mathbf{a}_N|\mathbf{t}_N)d\mathbf{a}_N\\
&= \frac{1}{p(\mathbf{t}_N)} \int
  p(a_{N+1}|\mathbf{a}) p(\mathbf{t}_N|a_{N+1},\mathbf{a}_N) d \mathbf{a}_N\\
&= \frac{1}{p(\mathbf{t}_N)} \int
  p(a_{N+1}|\mathbf{a})p(\mathbf{a}_N)  p(\mathbf{t}_N|\mathbf{a}_N)d\mathbf{a}_N\\
&= \int p(a_{N+1}|\mathbf{a})  p(\mathbf{a}_N|\mathbf{t}_N)d\mathbf{a}_N
\end{align*}
where $p(\mathbf{t}_N|a_{N+1},\mathbf{a}_N) = p(\mathbf{t}_N|\mathbf{a}_N) $
\begin{align}
p(a_{N+1}|\mathbf{a}_N) = \mathcal{N} (a_{N+1}|\mathbf{k}^T
  C_{N}^{-1}\mathbf{a}_N, c-\mathbf{k}^T C_{N}^{-1}\mathbf{k})
\end{align}

\begin{align*}
p(\mathbf{t}_{N}|\mathbf{a}_N) &= \prod_{n=1}^N \sigma(a_n)^{t_n}(1-\sigma(a_n))^{1-t_n}\\
&= \prod_{n=1}^N e^{a_nt_n} \sigma(-a_n)
\end{align*}
We then obtain the Laplace approximation by Taylor expanding the
logarithm of $p(a_N|t_N)$

\begin{align*}
\Psi(\mathbf{a}_N) &= \mathsf{ln} p(\mathbf{a}_N) + \mathsf{ln} p(\mathbf{t}_{N}|\mathbf{a}_N)\\
&=-\frac{1}{2}\mathbf{a}_N^TC_N^{-1}\mathbf{a}_N
  -\frac{N}{2}\mathsf{ln}(2\pi) -\frac{1}{2}\mathsf{ln}|C_N| +
  \mathbf{t}_N^T\mathbf{a}_N - \sum_{n=1}^N \mathsf{ln} (1+e^{a_n}) + const
\end{align*}
find the mode of the posterior distribution, and this requires that we
evaluate the gradient of $\Psi(a_N)$:
\begin{align}
\nabla \Psi(\mathbf{a}_N) = \mathbf{t}_N - \sigma_N - C_N^{-1}\mathbf{a}_N
\end{align}
$\sigma_N$ is a vector with elements $\sigma(a_n)$, we cannot simply find the mode by
setting this gradient to zero, because $\sigma_N$ depends nonlinearly on $a_N$, and so we
resort to an iterative scheme based on the Newton-Raphson method,
iterative reweighted least squares (IRLS) algorithm, This requires the second
derivatives of $\Psi(a_N)$
\begin{align}
  \nabla\nabla \Psi(a_N) = -\mathbf{W}_N - \mathbf{C}_N^{-1}
\end{align}
where$W_N$ is a diagonal matrix with elements $\sigma(a_n)(1−\sigma(a_n))$.
Using the Newton-Raphson formula, the iterative update equation for $a_N$
\begin{align}
a_N^{new} = C_N(I+W_NC_N)^{-1} {T_N -\sigma_N + W_Na_N}
\end{align}
These equations are iterated until they converge to the mode which we
denote by $a_N^*$. At the mode, the gradient $\nabla \Psi(a_N)$ will
vanish, and hence $a_N^*$ will satisfy
\begin{align}
a_N^* = C_N(t_N - \sigma_N)
\end{align}

\begin{align}
\mathbf{H} = - \nabla\nabla\Psi(a_N) = \mathbf{W_N + C_N^{-1}}
\end{align}

\begin{align}
q(a_n) = \mathcal{N}(a_n|a_N^*, H^{-1})\\
\mathbb{E}[a_{N+1}|\mathbf{t}_N] = k^T(t_N - \sigma_N)\\
var[a_{N+1}|\mathbf{t}_N] = c - k^T(W_N^{-1} + C_N)^{-1} \mathbf{k}
\end{align}
We also need to determine the parameters $\theta$ of the covariance function.

\begin{align}
p(\mathbf{t}_N |\theta) = \int p(\mathbf{t}_N|\mathbf{a}_N) p(\mathbf{a}_N |\theta)  d\mathbf{a}_N
\end{align}
\begin{align}
\mathsf{ln}p(\mathbf{t}_N |\theta) = \Psi(\mathbf{a}_N^*) -
  \frac{1}{2}\mathsf{ln}|\mathbf{W}_N+C_N^{-1}| + \frac{N}{2} \mathsf{ln}(2\pi)
\end{align}
where $\Psi(\mathbf{a}_N^*) = \mathsf{ln} p(\mathbf{a}_N^* |\theta) + p(\mathbf{t}_N |\mathbf{a}_N^*)$


\end{document}
