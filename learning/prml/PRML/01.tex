\documentclass[a4paper]{article}
\usepackage{xeCJK}
\setCJKmainfont{STKaiti}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Introduction}
\author{}
\maketitle

The SVM is a decision machine and so does not provide posterior probabilities.
\section{Polynomial Curve Fitting}

\begin{align}
y(x, \mathbf{w}) = w_0+w_1x+w_wx^2+...+w_Mx^M = \sum_{j=0}^M w_jx^j
\end{align}
error function
\begin{align}
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N (y(x_n,\mathbf{w})-t_n)^2
\end{align}

One technique that is often used to control the over-fitting
phenomenon in such cases is that of \textit{regularization}, which involves adding a penalty term to the error function.
\begin{align}
\widehat{E}(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N
  (y(x_n),\mathbf{w}-t_n)^2 + \frac{\lambda}{2} \|\mathbf{w}\|^2
\end{align}
$ \|\mathbf{w}\|^2 := \mathbf{w}^T\mathbf{w} = w_0^2+w_1^2+...+w_M^2 $
known in the statistics literature as \textit{shrinkage} methods
because they reduce the value of the coefficients.

\section{Probability Theory}
A key concept in the field of pattern recognition is that of
uncertainty.It arises both through noise on measurements, as well as through the finite size of data sets.
\textbf{sum rule and product rule}
\begin{align*}
p(X) = \sum_Y p(X,Y)\\
p(X,Y) = p(Y|X) p(X)
\end{align*}
Bayes' theorem, $p(X,Y) = p(Y,X) \Rightarrow p(Y|X) p(X) = p(X|Y)p(Y)$
\begin{align*}
p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}\\
p(X) = \sum_Y p(Y|X) p(X)
\end{align*}

if the joint distribution of two variables factorizes into the product
of the marginals, so that $p(X, Y) = p(X)p(Y)$, then X and Y are said
to be \textit{independent}. $p(Y |X) = p(Y)$

\subsection{Probability densities}
If the probability of a real-valued variable $x$ falling in the
interval $(x, x + \delta x)$ is given by $p(x)$ for $\delta x \to 0$, then
$p(x)$ is called the \textit{probability density} over $x$. The probability that $x$ will lie in an interval $(a, b)$ is then given by
\begin{align}
p(x \in (a,b)) = \int_a^b p(x) dx\\
p(x) \geq 0\\
\int_{-\infty}^{infty} p(x) = 1
\end{align}

The sum and product rules of probability, as well as Bayes' theorem,
apply equally to the case of probability densities:
\begin{align}
p(x) = \int p(x,y)dy\\
p(x,y) = p(y|x) p(x)
\end{align}

\subsection{Expectations and covariances}
For a discrete distribution, it is given by
\begin{align}
\mathbb{E}[f] = \sum_x p(x)f(x)
\end{align}
In the case of continuous variables
\begin{align}
\mathbb{E}[f] = \int p(x)f(x) dx
\end{align}
for a given finite number N:
\begin{align}
\mathbb{E}[f] \approx \sum_{n=1}^N f(x_n)
\end{align}


The variance of $f(x)$ is defined by
\begin{align}
var[f] = E[ (f(x) -E[f(x)])^2 ]\\
var[f] = E[f(x)^2] -E[f(x)]^2\\
var[x] = E[x^2] -E[x]^2
\end{align}
For two random variables $x$ and $y$, the covariance is defined by
\begin{align*}
cov[x,y] &= E_{x,y}[ (x-E[x])(y-E[y]) ]\\
&= E_{x,y}[xy] - E[x]E[y]
\end{align*}
In the case of two vectors of random variables $x$ and $y$, the covariance is a matrix
\begin{align*}
cov[\mathbf{x},\mathbf{y}] &= E_{x,y} [ (\mathbf{x}-E[\mathbf{x}])(\mathbf{y}-E[\mathbf{y}]) ]\\
&= E_{x,y}[\mathbf{x}\mathbf{y}^T] - E[\mathbf{x}]E[\mathbf{y}^T]
\end{align*}

\subsection{Bayesian probabilities}
在观察到数据之前,我们有一些关于参数$w$的假设,这以先验概率$p(w)$的形式给出,观测数据$D = {t_1,...,t_N}$的效果可以通过条件概率$p(D|w)$表达,贝叶斯定理的形式为
\begin{align}
  p(w|D) = { p(w)p(D|w) \over p(D) }
\end{align}
通过后验概率$p(w|D)$,在观测到$D$之后估计$w$的不确定性。$p(D|w)$由观测数据集$D$来估计,可以看做参数$w$的函数，叫做\textit{似然函数}（likelihood function）.
\emph{似然函数表达了在不同参数$w$下，观测数据可能出现的大小}。
Given this definition of likelihood, we can state Bayes' theorem in words.
\begin{align}
  posterior \propto likelihood \times prior \text{ (viewed as functions of $w$)}
\end{align}
分母$p(D)$是一个归一化常数,确保了后验概率$p(w|D)$是一个合理的概率密度,积分为1。可以用\emph{先验概率}和\emph{似然函数}来表达贝叶斯定理的分母$p(D)$
\begin{align}
  p(D) = \int p(w) p(D|w) \mathrm{d} w
\end{align}

\textbf{似然函数$p(D|w)$的贝叶斯观点和频率学家观点}
\begin{itemize}
\item frequentist：$w$被认为是一个固定的参数,它的值由某种形式的“估计”来确定,这个估计的误差通过考察可能的数据集$D$的概率分布来得到
\item Bayesian: 只有一个数据集$D$(即实际观测到的数据集),参数的不确定性通过$w$的概率分布来表达
\end{itemize}

\textbf{最大似然(maximum likelihood)}
频率学家广泛使用的一个估计是最大似然(maximum likelihood)估计,其中$w$的值是使似然函数$p(D|w)$达到最大值的$w$值
似然函数的负对数被叫做\emph{误差函数}(error function)

\subsection{Gaussian Distribution}

\subsection{Bayesian curve fitting}
\label{sec:1.2.6}

predictive distribution
\begin{align}
  p(t|x, \mathbf{X}, \mathbf{t}) = \int p(t|x,\mathbf{w}) p(\mathbf{w}|\mathbf{X},\mathbf{t}) \mathrm{d} \mathbf{w}
\end{align}
$p(t|x, w, \beta) = \mathcal{N}(t|y(x,w), \beta^{-1}) \text{ (1.60)}$
$p(\mathbf{w}|\mathbf{X},\mathbf{t})$ is posterior distribution over $w$, can be found by normalizing $p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta)p(\mathbf{w}|\alpha)$

\emph{Predictive distribution is given by a Gaussian}
\begin{align*}
  p(t|x, \mathbf{X}, \mathbf{t}) &= \mathcal{N}(t|m(x), s^2(x))\\
  m(x) &= \beta \phi(x)^T \mathbf{S} \sum_{n=1}^N \phi(x_n)t_n\\
  s^2(x) &= \beta^{-1} + \phi(x) S \phi(x)^T\\
  S^{-1} &= \alpha I + \beta  \sum_{n=1}^N \phi(x_n)\phi(x_n)
\end{align*}


\end{document}
