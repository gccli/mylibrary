\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Linear Models for Classification}
\author{}
\maketitle

The goal in classification is to take an input vector \textbf{x}
 and to assign it to one of \textbf{K} discrete classes $
 \mathcal{C}_k $ where $ \mathnormal{k} = 1,...,\mathnormal{K} $.
 The input space is thereby divided into \textit{decision regions} whose
 boundaries are called \textit{decision boundaries} or
 \textit{decision surfaces}.
1-of-K coding scheme in which $t$ is a vector of length
 K such that if the class is $C_j$ , then all elements $t_k$ of $t$
 are zero except element $t_j$ , which takes the value 1.

\begin{align}
\mathbf{t} = (0,0,0,1,0)^T
\end{align}

There are two different approaches to determining the conditional probabilities
 $p(C_k|\mathbf{x})$. representing them as parametric models and then
 optimizing the parameters using a training set
\begin{align}
p(C_k|\mathbf{x}) = \frac { p(\mathbf{x}|C_k) p(C_k) }{p(\mathbf{x})}
\end{align}

\begin{align}
y(\mathbf{x}) = f(\mathbf{w^T}\mathbf{x} + w_o)
\end{align}


\section{Discriminant Functions}
A discriminant is a function that takes an input vector $\mathbf{x}$
 and assigns it to one of $\mathbf{K}$ classes, denoted $C_k$.

\subsection{Two classes}

\begin{align}
y(\mathbf{x}) = \mathbf{w^T}\mathbf{x} + w_o
\end{align}
where $\mathbf{v}$ is called a weight vector, and $w_0$ is a bias

\begin{align}
\frac{\mathbf{w^T}\mathbf{x}}{\|\mathbf{w}\|} = -\frac{w_o}{\|\mathbf{w}\|}
\end{align}

an arbitrary point $x$ and let $x_{\bot}$ be its orthogonal
 projection onto the decision surface
\begin{align}
\mathbf{x} = x_{\bot} + r \frac{\mathbf{w}}{\|\mathbf{w}\|}
\end{align}

\begin{align}
r = \frac{y(\mathbf{x})}{\|\mathbf{w}\|}
\end{align}

\begin{align}
y(\mathbf{x}) = \widetilde{{\mathbf{w}}}^T \widetilde{{\mathbf{x}}}
\end{align}

\subsection{Multiple classes}
considering a single \textit{K}-class discriminant comprising
 \textit{K} linear functions of the form
\begin{align}
y_k(\mathbf{x}) = \mathbf{w}_k^T \mathbf{x} + \mathbf{w}_{k0}
\end{align}
and then assigning a point $\mathbf{x}$ to class $C_k$ if $y_k(x) >
y_j(x)$ for all $j \neq k$.The decision boundary between class $C_k$ and
 class $C_j$ is therefore given by $y_k(x) = y_j(x)$ and hence
 corresponds to a ($\mathit{D} - 1$)-dimensional hyperplane defined by
\begin{align}
(\mathbf{w}_k - \mathbf{w}_j)^T\mathbf{x} + (\mathbf{w}_{k0} -
  \mathbf{w}_{j0}) = 0
\end{align}
The decision regions of such a discriminant are always singly
 connected and convex. consider two points $x_A$ and $x_B$ both of
 which lie inside decision region $R_k$, any point $\widehat{x}$
 that lies on the line connecting $x_A$ and $x_B$ can be expressed in the form
\begin{align}
\widehat{x} = \lambda x_A + (1 - \lambda)x_B
\end{align}
\begin{align}
y_k(\widehat{x}) = \lambda y_k(x_A) + (1-\lambda)y_k(x_B)
\end{align}

\subsection{Least squares for classification}

\subsection{Fisher’s linear discriminant}

\begin{align}
y = \mathbf{w}^T \mathbf{x}
\end{align}

The Fisher criterion is defined to be the ratio of the between-class variance to
 the within-class variance and is given by

\begin{align}
\text{choose w so as to maximum } m_2 - m_1\\
m_k = \mathbf{w}^T \mathbf{m_k}\\
s_k^2 = \displaystyle\sum_{n \in C_k}(y_n - m_k)^2\\
\mathbf{J}(\mathbf{w}) = \frac{(m_2 - m_1)^2}{s_1^2+s_2^2}\\
\mathbf{J}(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S_B} \mathbf{w}}
  {\mathbf{w}^T \mathbf{S_W} \mathbf{w}}
\\
\end{align}
$S_B$ is the between-class covariance matrix,
 $S_W$ is the total within-class covariance matrix
\begin{align}
\mathbf{S_B} = (\mathbf{m_2} - \mathbf{m_1})(\mathbf{m_2} - \mathbf{m_1})^T\\
\mathbf{S_W} = \displaystyle\sum_{n \in C_k}(x_n - m_1) (x_n - m_1)^T +
\displaystyle\sum_{n \in C_k}(x_n - m_2) (x_n - m_2)^T
\end{align}
\begin{align}
\mathbf{w} \propto  \mathbf{S_W} (\mathbf{m_2} - \mathbf{m_1})
\end{align}

\subsection{Fisher’s discriminant for multiple classes}
\begin{align}
\mathbf{y} = \mathbf{W}^T \mathbf{x}
\end{align}

\subsection{The perceptron algorithm}
feature vector $\phi(x)$
\begin{align}
y(\mathbf{x}) = f(w^T\phi(x))\\
f(a) =
  \begin{cases}
   +1, & a \geq 0 \\
   -1, & a < 0
  \end{cases}
\end{align}

\textit{perceptron criterion}
seeking a weight vector $\mathbf{w}$ such that patterns $x_n$ in class $C_1$
 will have $w^T \phi(x_n) > 0$.whereas patterns $x_n$ in class $C_2$
 have $w^T \phi(x_n) < 0$. for target scheme $t \in {-1, +1}$, we would like all
 patterns to satisfied $w^T \phi(x_n)t_n > 0$. whereas for a misclassified
 pattern $x_n$ it tries to minimize the quantity $ - w^T \phi(x_n)t_n$
\begin{align}
E_p(w) = - \sum_{n \in \mathcal{W}}w^T \phi_n t_n  \text{   ($\mathcal{M}$
  denotes misclassified patterns)}
\\
w^{(\tau+1)} = w^{(\tau)} - \eta \nabla E_p(w) = w^{(\tau)} + \eta
  \phi_n t_n
\end{align}

\section{Probabilistic Discriminative Models}

\subsection{Logistic regression}
\begin{align}
p(C|\phi) = y(\phi) = \sigma(\mathbf{w}^T \phi)
\end{align}
with $p(C_2|\phi) = 1 - p(C_1|\phi)$, Here $\phi(.)$ is the logistic sigmoid,
 this model is known as \textit{logistic regression}
We now use maximum likelihood to determine the parameters of the
 logistic regression model. make use of the derivative of the logistic sigmoid:
\begin{align}
\tfrac{\partial \sigma}{\partial a} = \sigma (1-\sigma)
\end{align}
For a data set ${\phi_n, t_n}$, where $t_n \in {0, 1}$ and $\phi_n = \phi(x_n),
with n = 1,...,N$, the likelihood function can be written
\begin{align}
p(\mathbf{t}|\mathbf{w}) = \displaystyle\prod_{n-1}^{N} y_n^{t_n}(1-y_n)^{1-t_n}
\end{align}
$\mathbf{t} = \{t_1,...,t_N\}, y_n = p(C_1|\phi_n)$,
taking the negative logarithm of the likelihood, which gives the
 \textit{cross-entropy} error function in the form
\begin{align}
E(w) = -ln \mathit{p} (\mathbf{t}|w) = - \displaystyle \sum_{n=1}^N \{
  \mathit{t_n}ln\mathit{y_n} + (1-\mathit{t_n})ln(1-\mathit{y_n}) \}
\end{align}
where $y_n = \sigma(a_n) \text{ and } a_n = w^T \phi_n$. Taking the gradient of
 the error function with respect to w
\begin{align}
\nabla E(w) = \displaystyle \sum_{n=1}^N (\mathit{y_n} -\mathit{t_n}) \phi_n
\end{align}

\subsection{Iterative reweighted least squares}
error function can be minimized by an efficient iterative technique based on the
 \textit{Newton-Raphson} iterative optimization schemescheme, which uses a
 local quadratic approximation to the log likelihood function
The Newton-Raphson update, for minimizing a function $\mathit(E_w)$
\begin{align}
\mathbf{w} \gets \mathbf{w} - \mathbf{H}^{-1} \nabla \mathit{E}(\mathbf{w})
\end{align}
where $\mathbf{H}$ is the \textbf{Hessian} matrix whose elements comprise the second derivatives of $\mathit{E}(\mathbf{w})$ with respect to the components of $\mathbf{w}$
\begin{align}
\nabla \mathit{E}(\mathbf{w}) = \displaystyle \sum_{n=1}^N
  (\mathbf{w}^T \phi_n - t_n ) \phi_n = \mathbf{\Phi}^T \mathbf{\Phi}
  \mathbf{w} - \mathbf{\Phi}^T \mathbf{t}
\\
\mathbf{H} = \nabla \nabla E(\mathbf{w}) = \mathbf{\Phi}^T \mathbf{\Phi}
\\
\mathbf{w} \gets (\mathbf{\Phi}^T \mathbf{\Phi})^{-1}\mathbf{\Phi}^T \mathbf{t}
\end{align}
apply the Newton-Raphson update to the cross-entropy error function
 for the logistic regression model
\begin{align}
\nabla \mathit{E}(\mathbf{w}) = \displaystyle \sum_{n=1}^N
  (y_n - t_n ) \phi_n = \mathbf{\Phi}^T (\mathbf{y} - \mathbf{t})
\\
\mathbf{H} = \nabla \nabla E(\mathbf{w}) = \displaystyle
  \sum_{n=1}^N y_n(1-y_n)\phi_n\phi_n^T = \mathbf{\Phi}^T
  \mathbf{R} \mathbf{\Phi}
\\
R_{nn} = y_n(1-y_n) \text{ diagonal matrix }
\end{align}

The Newton-Raphson update formula for the logistic regression model then becomes
\begin{align*}
\mathbf{w} &= \mathbf{w} - (\mathbf{\Phi}^T \mathbf{R}
             \mathbf{\Phi})^{-1}\mathbf{\Phi}^T (\mathbf{y} -
             \mathbf{t})
\\
&= (\mathbf{\Phi}^T \mathbf{R} \mathbf{\Phi})^{-1} \{\mathbf{\Phi}^T
  \mathbf{R} \mathbf{\Phi} \mathbf{w} -
  \mathbf{\Phi}^T (\mathbf{y} - \mathbf{t}) \}
\\
&=(\mathbf{\Phi}^T \mathbf{R} \mathbf{\Phi})^{-1} \mathbf{\Phi}^T \mathbf{z}
\end{align}

\begin{align}
\mathbf{z} = \mathbf{\Phi} \mathbf{w} - \mathbf{R}^{-1}(\mathbf{y} - \mathbf{t})
\end{align}


\end{document}
