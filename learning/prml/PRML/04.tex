\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Linear Models for Classification}
\author{}
\maketitle

The goal in classification is to take an input vector \textbf{x} and to assign it to one of \textbf{K} discrete classes $\mathcal{C}_k $ where $ \mathnormal{k} = 1,...,\mathnormal{K} $.
 The input space is thereby divided into \textit{decision regions} whose
 boundaries are called \textit{decision boundaries} or
 \textit{decision surfaces}.
1-of-K coding scheme in which $t$ is a vector of length
 K such that if the class is $C_j$ , then all elements $t_k$ of $t$
 are zero except element $t_j$ , which takes the value 1.

\begin{align}
\mathbf{t} = (0,0,0,1,0)^T
\end{align}

There are two different approaches to determining the conditional probabilities
 $p(C_k|\mathbf{x})$. representing them as parametric models and then
 optimizing the parameters using a training set
\begin{align}
p(C_k|\mathbf{x}) = \frac { p(\mathbf{x}|C_k) p(C_k) }{p(\mathbf{x})}
\end{align}

\begin{align}
y(\mathbf{x}) = f(\mathbf{w^T}\mathbf{x} + w_o)
\end{align}


\section{Discriminant Functions}
A discriminant is a function that takes an input vector $\mathbf{x}$ and assigns it to one of $\mathbf{K}$ classes, denoted $C_k$.
\emph{线性判别函数}（linear discriminant function）,即那些决策面是超平面的判别函数
An input vector $x$ is assigned to class $C_1$ if $y(x) \geq 0$ and to class $C_2$ otherwise. The corresponding decision boundary is therefore defined by the relation $y(x) = 0$, which corresponds to a $(D − 1)$-dimensional hyperplane within the D-dimensional input space.
向量$w$与决策面内的任何向量都正交.

\subsection{Two classes}

\begin{align}
y(\mathbf{x}) = \mathbf{w^T}\mathbf{x} + w_o
\end{align}
where $\mathbf{v}$ is called a weight vector, and $w_0$ is a bias

\begin{align}
\frac{\mathbf{w^T}\mathbf{x}}{\|\mathbf{w}\|} = -\frac{w_o}{\|\mathbf{w}\|}
\end{align}

an arbitrary point $x$ and let $x_{\bot}$ be its orthogonal
 projection onto the decision surface
\begin{align}
\mathbf{x} = x_{\bot} + r \frac{\mathbf{w}}{\|\mathbf{w}\|}
\end{align}

\begin{align}
r = \frac{y(\mathbf{x})}{\|\mathbf{w}\|}
\end{align}

\begin{align}
y(\mathbf{x}) = \widetilde{{\mathbf{w}}}^T \widetilde{{\mathbf{x}}}
\end{align}

\subsection{Multiple classes}
considering a single \textit{K}-class discriminant comprising
 \textit{K} linear functions of the form
\begin{align}
y_k(\mathbf{x}) = \mathbf{w}_k^T \mathbf{x} + \mathbf{w}_{k0}
\end{align}
and then assigning a point $\mathbf{x}$ to class $C_k$ if $y_k(x) >
y_j(x)$ for all $j \neq k$.The decision boundary between class $C_k$ and
 class $C_j$ is therefore given by $y_k(x) = y_j(x)$ and hence
 corresponds to a ($\mathit{D} - 1$)-dimensional hyperplane defined by
\begin{align}
(\mathbf{w}_k - \mathbf{w}_j)^T\mathbf{x} + (\mathbf{w}_{k0} -
  \mathbf{w}_{j0}) = 0
\end{align}
The decision regions of such a discriminant are always singly
 connected and convex. consider two points $x_A$ and $x_B$ both of
 which lie inside decision region $R_k$, any point $\widehat{x}$
 that lies on the line connecting $x_A$ and $x_B$ can be expressed in the form
\begin{align}
\widehat{x} = \lambda x_A + (1 - \lambda)x_B
\end{align}
\begin{align}
y_k(\widehat{x}) = \lambda y_k(x_A) + (1-\lambda)y_k(x_B)
\end{align}

\subsection{Least squares for classification}

\subsection{Fisher’s linear discriminant}

\begin{align}
y = \mathbf{w}^T \mathbf{x}
\end{align}

The Fisher criterion is defined to be the ratio of the between-class variance to
 the within-class variance and is given by

\begin{align}
\text{choose w so as to maximum } m_2 - m_1\\
m_k = \mathbf{w}^T \mathbf{m_k}\\
s_k^2 = \displaystyle\sum_{n \in C_k}(y_n - m_k)^2\\
\mathbf{J}(\mathbf{w}) = \frac{(m_2 - m_1)^2}{s_1^2+s_2^2}\\
\mathbf{J}(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S_B} \mathbf{w}}
  {\mathbf{w}^T \mathbf{S_W} \mathbf{w}}
\\
\end{align}
$S_B$ is the between-class covariance matrix,
 $S_W$ is the total within-class covariance matrix
\begin{align}
\mathbf{S_B} = (\mathbf{m_2} - \mathbf{m_1})(\mathbf{m_2} - \mathbf{m_1})^T\\
\mathbf{S_W} = \displaystyle\sum_{n \in C_k}(x_n - m_1) (x_n - m_1)^T +
\displaystyle\sum_{n \in C_k}(x_n - m_2) (x_n - m_2)^T
\end{align}
上式称为Fisher\emph{线性判别函数}(Fisher linear discriminant),虽然严格来说它并不是一个判别函数,而是对于数据向一维投影的方向的一个具体选择

\begin{align}
\mathbf{w} \propto  \mathbf{S_W} (\mathbf{m_2} - \mathbf{m_1})
\end{align}

\subsection{Fisher’s discriminant for multiple classes}
\begin{align}
\mathbf{y} = \mathbf{W}^T \mathbf{x}
\end{align}

\subsection{The perceptron algorithm}
feature vector $\phi(x)$
\begin{align}
y(\mathbf{x}) = f(w^T\phi(x))\\
f(a) =
  \begin{cases}
   +1, & a \geq 0 \\
   -1, & a < 0
  \end{cases}
\end{align}

\textit{perceptron criterion}
seeking a weight vector $\mathbf{w}$ such that patterns $x_n$ in class $C_1$
 will have $w^T \phi(x_n) > 0$.whereas patterns $x_n$ in class $C_2$
 have $w^T \phi(x_n) < 0$. for target scheme $t \in {-1, +1}$, we would like all
 patterns to satisfied $w^T \phi(x_n)t_n > 0$. whereas for a misclassified
 pattern $x_n$ it tries to minimize the quantity $ - w^T \phi(x_n)t_n$
\begin{align}
E_p(w) = - \sum_{n \in \mathcal{W}}w^T \phi_n t_n  \text{   ($\mathcal{M}$
  denotes misclassified patterns)}
\\
w^{(\tau+1)} = w^{(\tau)} - \eta \nabla E_p(w) = w^{(\tau)} + \eta
  \phi_n t_n
\end{align}

\section{Probabilistic Generative Models}
\label{sec:4.2}
这种方法通过对类条件概率密度$p(x|C_k)$和类先验概率分布$p(C_k)$建模,然后使用这两个概率密度通过贝叶斯定理计算后验概率密度$p(C_k |x)$
考虑二分类
\begin{align*}
\label{e:4.2}
\mathbf{x}
p(C_1|\mathbf{x}) &= {p(\mathbf{x}|C_1)p(C_1) \ over p(\mathbf{x}|C_1)p(C_1) + p(\mathbf{x}|C_2)p(C_2)} \\
&= {1 \over 1+exp(-a)} = \sigma(a)\\
a &= \ln {p(\mathbf{x}|C_1)p(C_1) \ over  p(\mathbf{x}|C_2)p(C_2)}\\
\sigma(a) &= {1 \over \sigma{-a}} \textit{ (logistic sigmoid function)}\\
\sigma(-a) &= 1 - \sigma(a)
\end{align*}
logistic sigmoid 的反函数为
\begin{align}
  a = \ln {\sigma \over 1-\sigma}
\end{align}
称之为\emph{logit function},表示两类概率的对数比值

\subsection{Continuous inputs}
\label{sec:4.2.1}
$C_k$的类条件概率为
\begin{align*}
p(\mathbf{x}|C_k) = \mathcal{N}(x|\boldsymbol{\mu}_k, \Sigma)
\end{align*}
根据公式\ref{e:4.2}：
\begin{align*}
p(C_1|\mathbf{x}) &= \sigma(\mathbf{w}^T \mathbf{x} + w_0)\\
\mathbf{w} &= \Sigma^{-1} (\mu_1, \mu_2)\\
w_0 &= -{1 \over 2} \mu_1^T \Sigma^{-1}\mu_1 + {1 \over 2} \mu_2^T \Sigma^{-1}\mu_2 + \ln {p(C_1)/p(C_2)}
\end{align*}

\subsection{Maximum likelihood solution}
\label{sec:4.2.2}
一旦具体化了$p(x|C_k)$的参数化的函数形式,就能够使用最大似然法确定参数的值,以及先验类概率$p(C_k)$
考虑二分类情形：每个类别都有一个高斯类条件概率密度,且协方差矩阵相同.数据集${x_n, t_n}$,
把先验概率记作 $p(C_1) = \pi$, $p(C_2) = 1 − \pi$
likelihood function as follow:
\begin{align*}
  p(\mathbf{t},\mathbf{X}|\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) = \prod_{n=1}^N [\pi \mathcal{N}(x_n|\mu_1,\Sigma)]^{t_n} [(1-\pi) \mathcal{N}(x_n|\mu_2,\Sigma)]^{1-t_n}
\end{align*}

\begin{align}
  \pi = {N_1 \over N_1 + N_2}
\end{align}

\subsection{Discrete features}
\label{sec:4.2.3}
考察二元特征值$x_i \in \{0,1\}$

\subsection{Exponential family}
\label{sec:4.2.4}


\section{Probabilistic Discriminative Models}

\subsection{Fixed basis functions}
\label{sec:4.3.1}
先使用一个基函数向量 $\phi(x)$对输入变量进行一个\emph{固定的}非线性变换.
对于许多实际问题,类条件概率密度$p(x|C_k)$之间有着相当大的重叠,表明至少对于某些$x$的值,后验概率$p(C_k |x)$不等于$0$或$1$

\subsection{Logistic regression}
仍然考虑二分类，类别$C_1$的后验概率可以写成作用在特征向量$\phi$的线性函数上的 \emph{logistic sigmoid}函数的形式
\begin{align}
p(C|\phi) = y(\phi) = \sigma(\mathbf{w}^T \phi)
\end{align}
with $p(C_2|\phi) = 1 - p(C_1|\phi)$, Here $\phi(.)$ is the logistic sigmoid,
 this model is known as \textit{logistic regression}
We now use maximum likelihood to determine the parameters of the
 logistic regression model. make use of the derivative of the logistic sigmoid:
\begin{align}
\tfrac{\partial \sigma}{\partial a} = \sigma (1-\sigma)
\end{align}
For a data set ${\phi_n, t_n}$, where $t_n \in {0, 1}$ and $\phi_n = \phi(x_n),
with n = 1,...,N$, the likelihood function can be written
\begin{align}
p(\mathbf{t}|\mathbf{w}) = \displaystyle\prod_{n-1}^{N} y_n^{t_n}(1-y_n)^{1-t_n}
\end{align}
$\mathbf{t} = \{t_1,...,t_N\}, y_n = p(C_1|\phi_n)$,
taking the negative logarithm of the likelihood, which gives the
 \textit{cross-entropy} error function in the form
\begin{align}
E(w) = -ln \mathit{p} (\mathbf{t}|w) = - \displaystyle \sum_{n=1}^N \{
  \mathit{t_n}ln\mathit{y_n} + (1-\mathit{t_n})ln(1-\mathit{y_n}) \}
\end{align}
where $y_n = \sigma(a_n) \text{ and } a_n = w^T \phi_n$. Taking the gradient of
 the error function with respect to w
\begin{align}
\nabla E(w) = \displaystyle \sum_{n=1}^N (\mathit{y_n} -\mathit{t_n}) \phi_n
\end{align}

\subsection{Iterative reweighted least squares}
error function can be minimized by an efficient iterative technique based on the
 \textit{Newton-Raphson} iterative optimization schemescheme, which uses a
 local quadratic approximation to the log likelihood function
The Newton-Raphson update, for minimizing a function $\mathit(E_w)$
\begin{align}
\mathbf{w} \gets \mathbf{w} - \mathbf{H}^{-1} \nabla \mathit{E}(\mathbf{w})
\end{align}
where $\mathbf{H}$ is the \textbf{Hessian} matrix whose elements comprise the second derivatives of $\mathit{E}(\mathbf{w})$ with respect to the components of $\mathbf{w}$
\begin{align}
\nabla \mathit{E}(\mathbf{w}) = \displaystyle \sum_{n=1}^N
  (\mathbf{w}^T \phi_n - t_n ) \phi_n = \mathbf{\Phi}^T \mathbf{\Phi}
  \mathbf{w} - \mathbf{\Phi}^T \mathbf{t}
\\
\mathbf{H} = \nabla \nabla E(\mathbf{w}) = \mathbf{\Phi}^T \mathbf{\Phi}
\\
\mathbf{w} \gets (\mathbf{\Phi}^T \mathbf{\Phi})^{-1}\mathbf{\Phi}^T \mathbf{t}
\end{align}
apply the Newton-Raphson update to the cross-entropy error function
 for the logistic regression model
\begin{align}
\nabla \mathit{E}(\mathbf{w}) = \displaystyle \sum_{n=1}^N
  (y_n - t_n ) \phi_n = \mathbf{\Phi}^T (\mathbf{y} - \mathbf{t})
\\
\mathbf{H} = \nabla \nabla E(\mathbf{w}) = \displaystyle
  \sum_{n=1}^N y_n(1-y_n)\phi_n\phi_n^T = \mathbf{\Phi}^T
  \mathbf{R} \mathbf{\Phi}
\\
R_{nn} = y_n(1-y_n) \text{ diagonal matrix }
\end{align}

The Newton-Raphson update formula for the logistic regression model then becomes
\begin{align*}
\mathbf{w} &= \mathbf{w} - (\mathbf{\Phi}^T \mathbf{R}
             \mathbf{\Phi})^{-1}\mathbf{\Phi}^T (\mathbf{y} -
             \mathbf{t})
\\
&= (\mathbf{\Phi}^T \mathbf{R} \mathbf{\Phi})^{-1} \{\mathbf{\Phi}^T
  \mathbf{R} \mathbf{\Phi} \mathbf{w} -
  \mathbf{\Phi}^T (\mathbf{y} - \mathbf{t}) \}
\\
&=(\mathbf{\Phi}^T \mathbf{R} \mathbf{\Phi})^{-1} \mathbf{\Phi}^T \mathbf{z}
\end{align}

\begin{align}
\mathbf{z} = \mathbf{\Phi} \mathbf{w} - \mathbf{R}^{-1}(\mathbf{y} - \mathbf{t})
\end{align}


\delta

\end{document}
