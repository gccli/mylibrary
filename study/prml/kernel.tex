\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage[sc,osf,slantedGreek]{mathpazo}
\begin{document}
\title{Kernel Method}
\author{}
\maketitle

Many linear parametric models can be re-cast into an equivalent
 'dual representation' in which the predictions are also based on linear
 combinations of a kernel function evaluated at the training data points.
As we shall see, for models which are based on a fixed nonlinear feature
 space mapping $\phi(x)$, the kernel function is given
\begin{align}
\mathbf{k}(\mathbf{x}, \mathbf{x'}) = \phi(x)^T\phi(x')
\end{align}

\section{Dual Representations}
Many linear models for regression and classification can be reformulated in terms of
 a dual representation in which the kernel function arises naturally.
Here we consider a linear regression model whose parameters are determined by
minimizing a regularized sum-of-squares error function given by
\begin{align}
J(w) = \frac{1}{2} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}^2 + \frac{\lambda}{2}w^tw
\end{align}
If we set the gradient of $J(w)$ with respect to $\mathbf{w}$ equal to zero,
 we see that the solution for $\mathbf{w}$ takes the form of a linear
 combination of the vectors $\phi(x_n)$
\begin{align}
w = -\frac{1}{\lambda} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}\phi(x_n)
  = \sum_{n=1}^Na_n\phi(x_n) = \mathbf{\Phi}(x_n)\mathbf{a_n}
\end{align}
The nth row of $\mathbf{\Phi}$ is given by $\phi(x_n)^T$, $\mathbf{a}
= (a_1,...,a_n)$
\begin{align}
a_n = -\frac{1}{\lambda} \sum_{n=1}^{N} \{ w^T\phi(x_n) -t_n \}
\end{align}
Instead of working with the parameter vector $\mathbf{w}$, we can now
 reformulate the leastsquares algorithm in terms of the parameter vector
 $\mathbf{a}$, giving rise to a \textit{dual representation}.

\begin{align}
J(\mathbf{a}) = \frac{1}{2} \mathbf{a}^T \mathbf{\Phi} \mathbf{\Phi}^T
  \mathbf{\Phi} \mathbf{\Phi}^T \mathbf{a} - \mathbf{a}^T
  \mathbf{\Phi} \mathbf{\Phi}^T \mathbf{t} + \frac{1}{2} \mathbf{t}^T
  \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T \mathbf{\Phi}
  \mathbf{\Phi}^T \mathbf{a}
\end{align}
where $t = (t_1,...,t_N)^T$, We now define the \textit{Gram matrix}
$\mathbf{K} = \mathbf{\Phi} \mathbf{\Phi}^T$, which is an N × N symmetric matrix with elements
\begin{align}
\mathit{K}_{nm} = \phi(x_n)^T\phi(x_m) = k(x_n, x_m)
\end{align}

\begin{align}
J(\mathbf{a}) = \frac{1}{2} \mathbf{a}^T \mathbf{K} \mathbf{K} \mathbf{a} - \mathbf{a}^T
  \mathbf{K} \mathbf{t} + \frac{1}{2} \mathbf{t}^T
  \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T \mathbf{K} \mathbf{a}
\\
\mathbf{a} = (\mathbf{K} + \lambda \mathit{\mathbf{I}_N})^{-1} \mathbf{t}
\\
y(x) = w^T\phi(x) = a^T\Phi\phi(x) = \mathbf{k}(x)^T (\mathbf{K} +
  \lambda \mathit{\mathbf{I}_N})^{-1} \mathbf{t}
\end{align}
where we have defined the vector $\mathbf{k}(x)$ with elements $k_n(x) = k(x_n, x)$.

\section{Constructing Kernels}
Another commonly used kernel takes the form
\begin{align}
k(x, x') = exp(-\frac{\|x-x'\|^2}{2\sigma^2})\\
\|x-x'\|^2 = x^Tx+(x')^Tx' -2x^Tx'
\end{align}

One way to combine them is to use a generative model to define a kernel, and then use this kernel in a discriminative approach.
Given a generative model $p(x)$ we can define a kernel by
\begin{align}
k(x, x') = p(x)p(x')\\
k(x, x') = \sum_{i}p(x|i)p(x'|i)p(i)\\
k(x, x') = \int p(x|z)p(x'|z)p(z)dz
\end{align}
Now suppose that our data consists of ordered sequences of length $L$
so that an observation is given by $\mathbf{X} = {x_1,...,x_L}$. A
popular generative model for sequences is the hidden Markov
model. which expresses the distribution $p(\mathbf{X})$ as a
marginalization over a corresponding sequence of hidden states
$\mathbf{Z} = {z_1,...,z_L}$. We can use this approach to define a
kernel function measuring the similarity of two sequences $X$ and $X'$
by extending the mixture representation
\begin{align}
k(X, X') = \sum_{Z}p(X|Z)p(X'|Z)p(Z)
\end{align}

An alternative technique for using generative models to define kernel
functions is known as the \textit{Fisher kernel}. Consider a
parametric generative model $p(x|\theta)$ where $\theta$ denotes the
vector of parameters. The goal is to find a kernel that measures the
similarity of two input vectors $x$ and $x'$ induced by the generative model.
\textit{Fisher score}
\begin{align}
g(\theta, \mathbf{x}) = \nabla_\theta ln p(\mathbf{x}|\theta)\\
k(x, x') = g(\theta, \mathbf{x})^T F^{-1} g(\theta, \mathbf{x'})\\
F = \mathbb{E}_x [g(\theta, \mathbf{x}) g(\theta, \mathbf{x})^T]
\textit{ (Fisher information matrix)}
\end{align}
the expectation is with respect to $x$ under the distribution $p(x|\theta)$.
In practice, it is often infeasible to evaluate the Fisher information
matrix. One approach is simply to replace the expectation in the
definition of the Fisher information with the sample average

A final example of a kernel function is the sigmoidal kernel given by
\begin{align}
k(x,x') = tanh(a \mathbf{x}^T \mathbf{x}' + b)
\end{align}

\section{Radial Basis Function Networks}
One choice that has been widely used is that of radial basis
functions, which have the property that each basis function depends
only on the radial distance (typically Euclidean) from a centre
$\mu_j$ , so that $\phi_j(x) = h(\| − \mu_j\|) $.

\subsection{Nadaraya-Watson model}

\section{Gaussian Processes}

\subsection{Linear regression revisited}
Consider a model deﬁned in terms of a linear combination of $M$ ﬁxed
basis functions given by the elements of the vector $\phi(x)$ so that
\begin{align}
y(x) = \mathbf{w}^T\phi(\mathbf{x})\\
p(\mathbf{w}) = \mathcal{N} (\mathbf{w}|0, \alpha^{-1}\mathit{I})
\end{align}
we denote by the vector $\mathbf{y}$ with elements $y_n = y(x_n)$ for $n = 1,...,N$
\begin{align}
\mathbf{y} = \Phi \mathbf{w} \text{ $(\Phi_{nk} = \phi_k(x_n)) $ }\\
\mathbb{E}[y] = 0\\
cov[y] = \mathbb{E}[yy^T] = \Phi \mathbb{E}[ww^T] \Phi^T=
  \frac{1}{\alpha}\Phi\Phi^T = \mathbf{K}
\end{align}
where $K$ is the Gram matrix with elements
\begin{align}
\mathit{K}_{nm} = k(x_n, x_m) = \frac{1}{\alpha} \phi(x_n)^T\phi(x_m)
\end{align}

We can also deﬁne the kernel function directly, rather than indirectly
through a choice of basis function. Figure 6.4 shows samples of functions drawn from Gaussian processes for two different choices of kernel function. The ﬁrst of these is a
\end{document}
